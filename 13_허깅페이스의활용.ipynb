{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32207a5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27356805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987efe9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1153e6",
   "metadata": {},
   "source": [
    "# inference api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f26178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"hf-inference\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "result = client.translation(\n",
    "    \"Меня зовут Вольфганг и я живу в Берлине\",\n",
    "    model=\"Helsinki-NLP/opus-mt-ko-en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166da400",
   "metadata": {},
   "source": [
    "# pipeline()\n",
    "- task : 텍스트 기반 감성분석, 분류(제로 샷), 텍스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "006605c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.99980229139328}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('text-classification')   # task 지정\n",
    "classifier(\"Overall, it expresses familial and brotherly love.\\\n",
    "It's a shame there's no action from Yang Ziqing, but her active participation at 63 is impressive.\\\n",
    "Justin Chen's passionate performance as the eldest son stands out.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5317e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'Positive', 'score': 0.8681767582893372}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('text-classification', 'tabularisai/multilingual-sentiment-analysis')   # task 지정\n",
    "classifier(\"Overall, it expresses familial and brotherly love.\\\n",
    "It's a shame there's no action from Yang Ziqing, but her active participation at 63 is impressive.\\\n",
    "Justin Chen's passionate performance as the eldest son stands out.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42bc17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'Neutral', 'score': 0.4520818889141083}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('text-classification', 'tabularisai/multilingual-sentiment-analysis')   # task 지정\n",
    "classifier(\"빠른 편집과 감각적인 음악으로 세련된 액션과 스릴감을 전달한다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4dd00c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.15278035402297974,\n",
       "  'token': 9638,\n",
       "  'token_str': '이',\n",
       "  'sequence': '안녕하세요? 나는 이 모델입니다.'},\n",
       " {'score': 0.10853645205497742,\n",
       "  'token': 62592,\n",
       "  'token_str': '여자',\n",
       "  'sequence': '안녕하세요? 나는 여자 모델입니다.'},\n",
       " {'score': 0.07730689644813538,\n",
       "  'token': 108399,\n",
       "  'token_str': '가수',\n",
       "  'sequence': '안녕하세요? 나는 가수 모델입니다.'},\n",
       " {'score': 0.07643700391054153,\n",
       "  'token': 9283,\n",
       "  'token_str': '모',\n",
       "  'sequence': '안녕하세요? 나는 모 모델입니다.'},\n",
       " {'score': 0.03228118270635605,\n",
       "  'token': 102574,\n",
       "  'token_str': '프로',\n",
       "  'sequence': '안녕하세요? 나는 프로 모델입니다.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('fill-mask', 'bert-base-multilingual-cased')   # task 지정\n",
    "classifier(\"안녕하세요? 나는 [MASK] 모델입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b99a4",
   "metadata": {},
   "source": [
    "# question-answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23bdd2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 0 files: 0it [00:00, ?it/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Fetching 0 files: 0it [00:00, ?it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.8329089283943176,\n",
       " 'start': 37,\n",
       " 'end': 83,\n",
       " 'answer': 'psychological well-being and life satisfaction'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "qna = pipeline(\"question-answering\", model=\"consciousAI/question-answering-roberta-base-s-v2\")\n",
    "context = \"\"\"\n",
    "Happiness is a concept encompassing psychological well-being and life satisfaction, relating not only to short-term pleasures but also to long-term meaning, the quality of relationships, and a sense of accomplishment.\n",
    "\"\"\"\n",
    "qna(question=\"What is happiness?\", context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1b3d9",
   "metadata": {},
   "source": [
    "# ner 개체명 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ab571ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'NUM-B',\n",
       "  'score': np.float32(0.99999595),\n",
       "  'index': 11,\n",
       "  'word': '하나는',\n",
       "  'start': 27,\n",
       "  'end': 30}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "ner = pipeline(\"ner\", model=\"monologg/koelectra-base-finetuned-naver-ner\")\n",
    "ner(\"우리가 열 자극을 크게 인지하지 못하는 이유 중 하나는 시간적 특성 때문이다. 광노화는 비교적 빠르게 가시화되어 관리의 필요성을 인지하기 쉽지만, 열노화는 장시간 열 축적과 반복적인 노출로 서서히 누적되어 그 위험성을 간과하기 쉽다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f401cd",
   "metadata": {},
   "source": [
    "# summarization 문장 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ea6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = '''\n",
    "With tusks that each weigh over 100 pounds (45 kilograms) and often scrape the ground, “super tuskers” are elephants that live up to their name.\n",
    "This rare trait is the result of a genetic variation that causes the tusks to grow faster and longer than usual. While it gives the elephants a powerful and distinctive appearance, it also makes them vulnerable to poaching.\n",
    "Fewer than 30 of these giants remain in Africa, according to the Tsavo Trust, which monitors elephant populations in Kenya’s Tsavo Conservation Area.\n",
    "Most of the surviving super tuskers, also known as “great tuskers,” are found in southern Kenya, where Italian-born photographer Federico Veronesi has spent years tracking and photographing them. His new book, “Walk the Earth,” the culmination of years of work in Kenya and beyond, pays tribute to the elephants he calls “the last witnesses of a world before humans took over the Earth.”\n",
    "As a child growing up in Italy, Veronesi was captivated by African wildlife. That fascination led him to Kenya, where he eventually settled and began photographing the animals he had admired for so long.\n",
    "“Photography is almost like a medium through which I get to stay among the animals and express my love for them,” said Veronesi.\n",
    "Elephants became his central focus, not just for their grandeur, but for the way they seemed to embody the soul of Africa for him. “When you see an elephant walking through a savanna,” he reflected, “it’s so evocative, like everything about Africa is in that one scene.”\n",
    "The book is rooted in Veronesi’s encounters with some of Kenya’s most iconic elephants. One of the most significant came in 2010, when he first saw Tim, the legendary super tusker of Amboseli National Park.\n",
    "“We were waiting as the elephants made their daily walk from the woodlands to the marsh,” he remembered, “when this massive bull appeared with tusks so long, they nearly touched the ground.”\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b935f9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Fewer than 30 of these giants remain in Africa, according to the Tsavo Trust . Italian-born photographer Federico Veronesi has spent years tracking and photographing them . His new book, “Walk the Earth,” pays tribute to the elephants he calls “the last witnesses of a world before humans took over the Earth”'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summ = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
    "\n",
    "summ(ARTICLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6cbe83",
   "metadata": {},
   "source": [
    "# translation 한글 -> 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02571ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"One of the reasons why we don't perceive heat stimulation is because of its time-responsibility, which is relatively fast and easy to recognize the need for management, is because it slows down with long-term heat buildup and repeated exposure, making it easy to overlook the risk.\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "trans = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-ko-en\")\n",
    "\n",
    "trans(\"우리가 열 자극을 크게 인지하지 못하는 이유 중 하나는 시간적 특성 때문이다. 광노화는 비교적 빠르게 가시화되어 관리의 필요성을 인지하기 쉽지만, 열노화는 장시간 열 축적과 반복적인 노출로 서서히 누적되어 그 위험성을 간과하기 쉽다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b064210",
   "metadata": {},
   "source": [
    "# image-to-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902395d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d74a04f",
   "metadata": {},
   "source": [
    "# fine tuning을 위한 AutoClass 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fc5b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 120772.94 examples/s]\n",
      "Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 212594.58 examples/s]\n",
      "Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 390424.22 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('imdb')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ff8079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:05<00:00, 4777.81 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:05<00:00, 4670.95 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:11<00:00, 4444.82 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저 - 모델\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def preprocess_f(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_f, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55103290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19855560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정\n",
    "from transformers import TrainingArguments, Trainer\n",
    "tr_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=15,\n",
    "    per_device_eval_batch_size=15,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=tr_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "938359dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8335' max='8335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8335/8335 53:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.186900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.098500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.090900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.034400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8335, training_loss=0.1193792601724406, metrics={'train_runtime': 3221.4543, 'train_samples_per_second': 38.802, 'train_steps_per_second': 2.587, 'total_flos': 1.6558424832e+16, 'train_loss': 0.1193792601724406, 'epoch': 5.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d77d362a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1667' max='1667' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1667/1667 03:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_distilbert_imdb\\\\tokenizer_config.json',\n",
       " './fine_tuned_distilbert_imdb\\\\special_tokens_map.json',\n",
       " './fine_tuned_distilbert_imdb\\\\vocab.txt',\n",
       " './fine_tuned_distilbert_imdb\\\\added_tokens.json',\n",
       " './fine_tuned_distilbert_imdb\\\\tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "trainer.evaluate()\n",
    "\n",
    "# 저장\n",
    "model_path = './fine_tuned_distilbert_imdb'\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "731ae120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장 모델 로드\n",
    "\n",
    "# 토크나이저 먼저 로드 from_pretrained(저장경로)\n",
    "tokenizer = AutoTokenizer.from_pretrained('./fine_tuned_distilbert_imdb')\n",
    "# 모델 로드\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./fine_tuned_distilbert_imdb')\n",
    "# 모델.eval()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "447ed838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  3185,  2001,  7078, 10392,   999,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장 모델로 추론하기\n",
    "input_text = 'This movie was absolutely fantastic!'\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors='pt')  # pytorch 형식으로 텐서를 리턴\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecbe04b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 결과: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():  # 역전파x, 추론 only\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "predict_class_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "print(f\"예측 결과: {predict_class_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beb156d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cuda_transformer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
